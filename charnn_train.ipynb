{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whole Project Folder has to be given to students\n",
    "# Student have to install anaconda \n",
    "# Students also need the envrionemnt file \n",
    "# They activate the environment they cd to Project folder \n",
    "# THEN RUN \"jupyter notebook\"\n",
    "# Browser windows opens \n",
    "# And they can run the cells \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model - train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "from six.moves import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    data_dir = 'data/space'\n",
    "    save_dir = 'save'\n",
    "    log_dir = 'logs'\n",
    "    save_every = 1000\n",
    "    init_from = None\n",
    "    model = 'lstm'\n",
    "    rnn_size = 128\n",
    "    num_layers = 2 \n",
    "    seq_length = 50 \n",
    "    batch_size = 50 \n",
    "    num_epochs = 2 \n",
    "    grad_clip = 5.\n",
    "    learning_rate = 0.002 \n",
    "    decay_rate = 0.97 \n",
    "    output_keep_prob = 1.0 \n",
    "    input_keep_prob = 1.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from utils import TextLoader\n",
    "from model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading preprocessed files\n",
      "WARNING:tensorflow:From C:\\Users\\ljwag\\char-rnn-tensorflow\\model.py:30: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From C:\\Users\\ljwag\\char-rnn-tensorflow\\model.py:36: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From C:\\Users\\ljwag\\char-rnn-tensorflow\\model.py:39: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ljwag\\char-rnn-tensorflow\\model.py:46: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ljwag\\char-rnn-tensorflow\\model.py:47: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ljwag\\char-rnn-tensorflow\\model.py:57: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\ljwag\\anaconda3\\envs\\charnn-tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\ljwag\\anaconda3\\envs\\charnn-tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\ljwag\\char-rnn-tensorflow\\model.py:86: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ljwag\\anaconda3\\envs\\charnn-tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\ljwag\\char-rnn-tensorflow\\model.py:92: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ljwag\\char-rnn-tensorflow\\model.py:98: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ljwag\\char-rnn-tensorflow\\model.py:100: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "0/298 (epoch 0), train_loss = 4.439, time/batch = 0.337\n",
      "model saved to save\\model.ckpt\n",
      "1/298 (epoch 0), train_loss = 4.404, time/batch = 0.029\n",
      "2/298 (epoch 0), train_loss = 4.326, time/batch = 0.028\n",
      "3/298 (epoch 0), train_loss = 4.062, time/batch = 0.028\n",
      "4/298 (epoch 0), train_loss = 3.638, time/batch = 0.028\n",
      "5/298 (epoch 0), train_loss = 3.415, time/batch = 0.027\n",
      "6/298 (epoch 0), train_loss = 3.309, time/batch = 0.027\n",
      "7/298 (epoch 0), train_loss = 3.220, time/batch = 0.027\n",
      "8/298 (epoch 0), train_loss = 3.207, time/batch = 0.031\n",
      "9/298 (epoch 0), train_loss = 3.179, time/batch = 0.028\n",
      "10/298 (epoch 0), train_loss = 3.203, time/batch = 0.029\n",
      "11/298 (epoch 0), train_loss = 3.142, time/batch = 0.027\n",
      "12/298 (epoch 0), train_loss = 3.066, time/batch = 0.027\n",
      "13/298 (epoch 0), train_loss = 3.102, time/batch = 0.028\n",
      "14/298 (epoch 0), train_loss = 3.154, time/batch = 0.028\n",
      "15/298 (epoch 0), train_loss = 3.079, time/batch = 0.028\n",
      "16/298 (epoch 0), train_loss = 3.126, time/batch = 0.031\n",
      "17/298 (epoch 0), train_loss = 3.064, time/batch = 0.028\n",
      "18/298 (epoch 0), train_loss = 3.043, time/batch = 0.028\n",
      "19/298 (epoch 0), train_loss = 3.087, time/batch = 0.028\n",
      "20/298 (epoch 0), train_loss = 3.038, time/batch = 0.032\n",
      "21/298 (epoch 0), train_loss = 3.074, time/batch = 0.038\n",
      "22/298 (epoch 0), train_loss = 3.084, time/batch = 0.045\n",
      "23/298 (epoch 0), train_loss = 3.097, time/batch = 0.042\n",
      "24/298 (epoch 0), train_loss = 3.105, time/batch = 0.041\n",
      "25/298 (epoch 0), train_loss = 3.090, time/batch = 0.041\n",
      "26/298 (epoch 0), train_loss = 3.107, time/batch = 0.041\n",
      "27/298 (epoch 0), train_loss = 3.128, time/batch = 0.032\n",
      "28/298 (epoch 0), train_loss = 3.076, time/batch = 0.028\n",
      "29/298 (epoch 0), train_loss = 3.053, time/batch = 0.030\n",
      "30/298 (epoch 0), train_loss = 3.115, time/batch = 0.028\n",
      "31/298 (epoch 0), train_loss = 3.118, time/batch = 0.028\n",
      "32/298 (epoch 0), train_loss = 3.113, time/batch = 0.028\n",
      "33/298 (epoch 0), train_loss = 3.081, time/batch = 0.027\n",
      "34/298 (epoch 0), train_loss = 3.064, time/batch = 0.028\n",
      "35/298 (epoch 0), train_loss = 3.099, time/batch = 0.028\n",
      "36/298 (epoch 0), train_loss = 3.079, time/batch = 0.029\n",
      "37/298 (epoch 0), train_loss = 3.030, time/batch = 0.034\n",
      "38/298 (epoch 0), train_loss = 3.085, time/batch = 0.035\n",
      "39/298 (epoch 0), train_loss = 3.089, time/batch = 0.033\n",
      "40/298 (epoch 0), train_loss = 3.040, time/batch = 0.031\n",
      "41/298 (epoch 0), train_loss = 3.033, time/batch = 0.033\n",
      "42/298 (epoch 0), train_loss = 3.071, time/batch = 0.032\n",
      "43/298 (epoch 0), train_loss = 3.069, time/batch = 0.031\n",
      "44/298 (epoch 0), train_loss = 3.041, time/batch = 0.032\n",
      "45/298 (epoch 0), train_loss = 3.081, time/batch = 0.027\n",
      "46/298 (epoch 0), train_loss = 3.048, time/batch = 0.027\n",
      "47/298 (epoch 0), train_loss = 3.087, time/batch = 0.027\n",
      "48/298 (epoch 0), train_loss = 3.078, time/batch = 0.027\n",
      "49/298 (epoch 0), train_loss = 3.028, time/batch = 0.028\n",
      "50/298 (epoch 0), train_loss = 3.069, time/batch = 0.028\n",
      "51/298 (epoch 0), train_loss = 3.070, time/batch = 0.027\n",
      "52/298 (epoch 0), train_loss = 3.073, time/batch = 0.030\n",
      "53/298 (epoch 0), train_loss = 3.069, time/batch = 0.028\n",
      "54/298 (epoch 0), train_loss = 3.077, time/batch = 0.028\n",
      "55/298 (epoch 0), train_loss = 3.054, time/batch = 0.028\n",
      "56/298 (epoch 0), train_loss = 3.051, time/batch = 0.028\n",
      "57/298 (epoch 0), train_loss = 3.056, time/batch = 0.028\n",
      "58/298 (epoch 0), train_loss = 3.055, time/batch = 0.027\n",
      "59/298 (epoch 0), train_loss = 3.010, time/batch = 0.028\n",
      "60/298 (epoch 0), train_loss = 2.976, time/batch = 0.035\n",
      "61/298 (epoch 0), train_loss = 2.997, time/batch = 0.029\n",
      "62/298 (epoch 0), train_loss = 3.020, time/batch = 0.028\n",
      "63/298 (epoch 0), train_loss = 3.008, time/batch = 0.028\n",
      "64/298 (epoch 0), train_loss = 2.974, time/batch = 0.028\n",
      "65/298 (epoch 0), train_loss = 2.980, time/batch = 0.028\n",
      "66/298 (epoch 0), train_loss = 2.975, time/batch = 0.028\n",
      "67/298 (epoch 0), train_loss = 2.980, time/batch = 0.028\n",
      "68/298 (epoch 0), train_loss = 2.961, time/batch = 0.033\n",
      "69/298 (epoch 0), train_loss = 2.973, time/batch = 0.041\n",
      "70/298 (epoch 0), train_loss = 3.037, time/batch = 0.028\n",
      "71/298 (epoch 0), train_loss = 2.930, time/batch = 0.028\n",
      "72/298 (epoch 0), train_loss = 2.922, time/batch = 0.032\n",
      "73/298 (epoch 0), train_loss = 2.891, time/batch = 0.028\n",
      "74/298 (epoch 0), train_loss = 2.884, time/batch = 0.028\n",
      "75/298 (epoch 0), train_loss = 2.861, time/batch = 0.028\n",
      "76/298 (epoch 0), train_loss = 2.865, time/batch = 0.028\n",
      "77/298 (epoch 0), train_loss = 2.828, time/batch = 0.028\n",
      "78/298 (epoch 0), train_loss = 2.836, time/batch = 0.028\n",
      "79/298 (epoch 0), train_loss = 2.799, time/batch = 0.028\n",
      "80/298 (epoch 0), train_loss = 2.808, time/batch = 0.027\n",
      "81/298 (epoch 0), train_loss = 2.737, time/batch = 0.028\n",
      "82/298 (epoch 0), train_loss = 2.769, time/batch = 0.028\n",
      "83/298 (epoch 0), train_loss = 2.766, time/batch = 0.029\n",
      "84/298 (epoch 0), train_loss = 2.731, time/batch = 0.028\n",
      "85/298 (epoch 0), train_loss = 2.748, time/batch = 0.028\n",
      "86/298 (epoch 0), train_loss = 2.685, time/batch = 0.028\n",
      "87/298 (epoch 0), train_loss = 2.689, time/batch = 0.028\n",
      "88/298 (epoch 0), train_loss = 2.669, time/batch = 0.027\n",
      "89/298 (epoch 0), train_loss = 2.670, time/batch = 0.028\n",
      "90/298 (epoch 0), train_loss = 2.681, time/batch = 0.028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/298 (epoch 0), train_loss = 2.682, time/batch = 0.029\n",
      "92/298 (epoch 0), train_loss = 2.661, time/batch = 0.028\n",
      "93/298 (epoch 0), train_loss = 2.737, time/batch = 0.028\n",
      "94/298 (epoch 0), train_loss = 2.700, time/batch = 0.028\n",
      "95/298 (epoch 0), train_loss = 2.680, time/batch = 0.028\n",
      "96/298 (epoch 0), train_loss = 2.637, time/batch = 0.029\n",
      "97/298 (epoch 0), train_loss = 2.663, time/batch = 0.028\n",
      "98/298 (epoch 0), train_loss = 2.625, time/batch = 0.028\n",
      "99/298 (epoch 0), train_loss = 2.721, time/batch = 0.028\n",
      "100/298 (epoch 0), train_loss = 2.585, time/batch = 0.028\n",
      "101/298 (epoch 0), train_loss = 2.552, time/batch = 0.028\n",
      "102/298 (epoch 0), train_loss = 2.612, time/batch = 0.028\n",
      "103/298 (epoch 0), train_loss = 2.536, time/batch = 0.028\n",
      "104/298 (epoch 0), train_loss = 2.550, time/batch = 0.028\n",
      "105/298 (epoch 0), train_loss = 2.531, time/batch = 0.028\n",
      "106/298 (epoch 0), train_loss = 2.533, time/batch = 0.028\n",
      "107/298 (epoch 0), train_loss = 2.531, time/batch = 0.028\n",
      "108/298 (epoch 0), train_loss = 2.565, time/batch = 0.030\n",
      "109/298 (epoch 0), train_loss = 2.541, time/batch = 0.030\n",
      "110/298 (epoch 0), train_loss = 2.536, time/batch = 0.029\n",
      "111/298 (epoch 0), train_loss = 2.609, time/batch = 0.032\n",
      "112/298 (epoch 0), train_loss = 2.545, time/batch = 0.030\n",
      "113/298 (epoch 0), train_loss = 2.518, time/batch = 0.030\n",
      "114/298 (epoch 0), train_loss = 2.537, time/batch = 0.032\n",
      "115/298 (epoch 0), train_loss = 2.484, time/batch = 0.031\n",
      "116/298 (epoch 0), train_loss = 2.523, time/batch = 0.029\n",
      "117/298 (epoch 0), train_loss = 2.537, time/batch = 0.028\n",
      "118/298 (epoch 0), train_loss = 2.542, time/batch = 0.027\n",
      "119/298 (epoch 0), train_loss = 2.526, time/batch = 0.028\n",
      "120/298 (epoch 0), train_loss = 2.476, time/batch = 0.028\n",
      "121/298 (epoch 0), train_loss = 2.440, time/batch = 0.028\n",
      "122/298 (epoch 0), train_loss = 2.472, time/batch = 0.029\n",
      "123/298 (epoch 0), train_loss = 2.501, time/batch = 0.028\n",
      "124/298 (epoch 0), train_loss = 2.460, time/batch = 0.028\n",
      "125/298 (epoch 0), train_loss = 2.442, time/batch = 0.028\n",
      "126/298 (epoch 0), train_loss = 2.523, time/batch = 0.027\n",
      "127/298 (epoch 0), train_loss = 2.453, time/batch = 0.028\n",
      "128/298 (epoch 0), train_loss = 2.509, time/batch = 0.029\n",
      "129/298 (epoch 0), train_loss = 2.483, time/batch = 0.028\n",
      "130/298 (epoch 0), train_loss = 2.496, time/batch = 0.029\n",
      "131/298 (epoch 0), train_loss = 2.480, time/batch = 0.028\n",
      "132/298 (epoch 0), train_loss = 2.482, time/batch = 0.029\n",
      "133/298 (epoch 0), train_loss = 2.521, time/batch = 0.028\n",
      "134/298 (epoch 0), train_loss = 2.489, time/batch = 0.028\n",
      "135/298 (epoch 0), train_loss = 2.448, time/batch = 0.028\n",
      "136/298 (epoch 0), train_loss = 2.499, time/batch = 0.029\n",
      "137/298 (epoch 0), train_loss = 2.486, time/batch = 0.029\n",
      "138/298 (epoch 0), train_loss = 2.450, time/batch = 0.036\n",
      "139/298 (epoch 0), train_loss = 2.511, time/batch = 0.028\n",
      "140/298 (epoch 0), train_loss = 2.495, time/batch = 0.028\n",
      "141/298 (epoch 0), train_loss = 2.489, time/batch = 0.028\n",
      "142/298 (epoch 0), train_loss = 2.515, time/batch = 0.028\n",
      "143/298 (epoch 0), train_loss = 2.432, time/batch = 0.029\n",
      "144/298 (epoch 0), train_loss = 2.488, time/batch = 0.028\n",
      "145/298 (epoch 0), train_loss = 2.448, time/batch = 0.028\n",
      "146/298 (epoch 0), train_loss = 2.456, time/batch = 0.028\n",
      "147/298 (epoch 0), train_loss = 2.435, time/batch = 0.028\n",
      "148/298 (epoch 0), train_loss = 2.467, time/batch = 0.028\n",
      "149/298 (epoch 1), train_loss = 2.546, time/batch = 0.028\n",
      "150/298 (epoch 1), train_loss = 2.507, time/batch = 0.028\n",
      "151/298 (epoch 1), train_loss = 2.485, time/batch = 0.029\n",
      "152/298 (epoch 1), train_loss = 2.417, time/batch = 0.029\n",
      "153/298 (epoch 1), train_loss = 2.469, time/batch = 0.027\n",
      "154/298 (epoch 1), train_loss = 2.436, time/batch = 0.028\n",
      "155/298 (epoch 1), train_loss = 2.444, time/batch = 0.028\n",
      "156/298 (epoch 1), train_loss = 2.419, time/batch = 0.028\n",
      "157/298 (epoch 1), train_loss = 2.473, time/batch = 0.028\n",
      "158/298 (epoch 1), train_loss = 2.441, time/batch = 0.029\n",
      "159/298 (epoch 1), train_loss = 2.546, time/batch = 0.029\n",
      "160/298 (epoch 1), train_loss = 2.471, time/batch = 0.028\n",
      "161/298 (epoch 1), train_loss = 2.386, time/batch = 0.028\n",
      "162/298 (epoch 1), train_loss = 2.436, time/batch = 0.027\n",
      "163/298 (epoch 1), train_loss = 2.460, time/batch = 0.028\n",
      "164/298 (epoch 1), train_loss = 2.410, time/batch = 0.029\n",
      "165/298 (epoch 1), train_loss = 2.427, time/batch = 0.028\n",
      "166/298 (epoch 1), train_loss = 2.380, time/batch = 0.028\n",
      "167/298 (epoch 1), train_loss = 2.347, time/batch = 0.029\n",
      "168/298 (epoch 1), train_loss = 2.397, time/batch = 0.028\n",
      "169/298 (epoch 1), train_loss = 2.374, time/batch = 0.028\n",
      "170/298 (epoch 1), train_loss = 2.398, time/batch = 0.028\n",
      "171/298 (epoch 1), train_loss = 2.423, time/batch = 0.028\n",
      "172/298 (epoch 1), train_loss = 2.440, time/batch = 0.028\n",
      "173/298 (epoch 1), train_loss = 2.411, time/batch = 0.029\n",
      "174/298 (epoch 1), train_loss = 2.439, time/batch = 0.029\n",
      "175/298 (epoch 1), train_loss = 2.435, time/batch = 0.028\n",
      "176/298 (epoch 1), train_loss = 2.482, time/batch = 0.028\n",
      "177/298 (epoch 1), train_loss = 2.397, time/batch = 0.028\n",
      "178/298 (epoch 1), train_loss = 2.366, time/batch = 0.028\n",
      "179/298 (epoch 1), train_loss = 2.394, time/batch = 0.028\n",
      "180/298 (epoch 1), train_loss = 2.423, time/batch = 0.027\n",
      "181/298 (epoch 1), train_loss = 2.445, time/batch = 0.029\n",
      "182/298 (epoch 1), train_loss = 2.384, time/batch = 0.028\n",
      "183/298 (epoch 1), train_loss = 2.386, time/batch = 0.029\n",
      "184/298 (epoch 1), train_loss = 2.437, time/batch = 0.028\n",
      "185/298 (epoch 1), train_loss = 2.384, time/batch = 0.029\n",
      "186/298 (epoch 1), train_loss = 2.342, time/batch = 0.028\n",
      "187/298 (epoch 1), train_loss = 2.386, time/batch = 0.027\n",
      "188/298 (epoch 1), train_loss = 2.410, time/batch = 0.028\n",
      "189/298 (epoch 1), train_loss = 2.336, time/batch = 0.029\n",
      "190/298 (epoch 1), train_loss = 2.351, time/batch = 0.028\n",
      "191/298 (epoch 1), train_loss = 2.376, time/batch = 0.029\n",
      "192/298 (epoch 1), train_loss = 2.386, time/batch = 0.029\n",
      "193/298 (epoch 1), train_loss = 2.339, time/batch = 0.028\n",
      "194/298 (epoch 1), train_loss = 2.381, time/batch = 0.028\n",
      "195/298 (epoch 1), train_loss = 2.325, time/batch = 0.028\n",
      "196/298 (epoch 1), train_loss = 2.380, time/batch = 0.028\n",
      "197/298 (epoch 1), train_loss = 2.360, time/batch = 0.028\n",
      "198/298 (epoch 1), train_loss = 2.328, time/batch = 0.029\n",
      "199/298 (epoch 1), train_loss = 2.342, time/batch = 0.028\n",
      "200/298 (epoch 1), train_loss = 2.373, time/batch = 0.028\n",
      "201/298 (epoch 1), train_loss = 2.397, time/batch = 0.028\n",
      "202/298 (epoch 1), train_loss = 2.365, time/batch = 0.028\n",
      "203/298 (epoch 1), train_loss = 2.381, time/batch = 0.028\n",
      "204/298 (epoch 1), train_loss = 2.334, time/batch = 0.028\n",
      "205/298 (epoch 1), train_loss = 2.391, time/batch = 0.029\n",
      "206/298 (epoch 1), train_loss = 2.377, time/batch = 0.028\n",
      "207/298 (epoch 1), train_loss = 2.404, time/batch = 0.028\n",
      "208/298 (epoch 1), train_loss = 2.312, time/batch = 0.028\n",
      "209/298 (epoch 1), train_loss = 2.299, time/batch = 0.028\n",
      "210/298 (epoch 1), train_loss = 2.336, time/batch = 0.028\n",
      "211/298 (epoch 1), train_loss = 2.381, time/batch = 0.028\n",
      "212/298 (epoch 1), train_loss = 2.383, time/batch = 0.028\n",
      "213/298 (epoch 1), train_loss = 2.327, time/batch = 0.029\n",
      "214/298 (epoch 1), train_loss = 2.354, time/batch = 0.028\n",
      "215/298 (epoch 1), train_loss = 2.369, time/batch = 0.029\n",
      "216/298 (epoch 1), train_loss = 2.394, time/batch = 0.029\n",
      "217/298 (epoch 1), train_loss = 2.344, time/batch = 0.028\n",
      "218/298 (epoch 1), train_loss = 2.399, time/batch = 0.028\n",
      "219/298 (epoch 1), train_loss = 2.500, time/batch = 0.029\n",
      "220/298 (epoch 1), train_loss = 2.381, time/batch = 0.028\n",
      "221/298 (epoch 1), train_loss = 2.352, time/batch = 0.028\n",
      "222/298 (epoch 1), train_loss = 2.324, time/batch = 0.028\n",
      "223/298 (epoch 1), train_loss = 2.333, time/batch = 0.028\n",
      "224/298 (epoch 1), train_loss = 2.294, time/batch = 0.028\n",
      "225/298 (epoch 1), train_loss = 2.348, time/batch = 0.028\n",
      "226/298 (epoch 1), train_loss = 2.309, time/batch = 0.028\n",
      "227/298 (epoch 1), train_loss = 2.330, time/batch = 0.028\n",
      "228/298 (epoch 1), train_loss = 2.328, time/batch = 0.028\n",
      "229/298 (epoch 1), train_loss = 2.320, time/batch = 0.029\n",
      "230/298 (epoch 1), train_loss = 2.274, time/batch = 0.028\n",
      "231/298 (epoch 1), train_loss = 2.322, time/batch = 0.028\n",
      "232/298 (epoch 1), train_loss = 2.294, time/batch = 0.029\n",
      "233/298 (epoch 1), train_loss = 2.275, time/batch = 0.028\n",
      "234/298 (epoch 1), train_loss = 2.331, time/batch = 0.029\n",
      "235/298 (epoch 1), train_loss = 2.297, time/batch = 0.028\n",
      "236/298 (epoch 1), train_loss = 2.293, time/batch = 0.028\n",
      "237/298 (epoch 1), train_loss = 2.252, time/batch = 0.029\n",
      "238/298 (epoch 1), train_loss = 2.290, time/batch = 0.028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239/298 (epoch 1), train_loss = 2.306, time/batch = 0.028\n",
      "240/298 (epoch 1), train_loss = 2.306, time/batch = 0.028\n",
      "241/298 (epoch 1), train_loss = 2.295, time/batch = 0.028\n",
      "242/298 (epoch 1), train_loss = 2.347, time/batch = 0.029\n",
      "243/298 (epoch 1), train_loss = 2.342, time/batch = 0.029\n",
      "244/298 (epoch 1), train_loss = 2.309, time/batch = 0.028\n",
      "245/298 (epoch 1), train_loss = 2.283, time/batch = 0.028\n",
      "246/298 (epoch 1), train_loss = 2.320, time/batch = 0.028\n",
      "247/298 (epoch 1), train_loss = 2.285, time/batch = 0.028\n",
      "248/298 (epoch 1), train_loss = 2.381, time/batch = 0.028\n",
      "249/298 (epoch 1), train_loss = 2.247, time/batch = 0.028\n",
      "250/298 (epoch 1), train_loss = 2.230, time/batch = 0.028\n",
      "251/298 (epoch 1), train_loss = 2.283, time/batch = 0.028\n",
      "252/298 (epoch 1), train_loss = 2.223, time/batch = 0.028\n",
      "253/298 (epoch 1), train_loss = 2.246, time/batch = 0.028\n",
      "254/298 (epoch 1), train_loss = 2.239, time/batch = 0.028\n",
      "255/298 (epoch 1), train_loss = 2.243, time/batch = 0.029\n",
      "256/298 (epoch 1), train_loss = 2.228, time/batch = 0.028\n",
      "257/298 (epoch 1), train_loss = 2.251, time/batch = 0.028\n",
      "258/298 (epoch 1), train_loss = 2.244, time/batch = 0.027\n",
      "259/298 (epoch 1), train_loss = 2.204, time/batch = 0.029\n",
      "260/298 (epoch 1), train_loss = 2.306, time/batch = 0.029\n",
      "261/298 (epoch 1), train_loss = 2.265, time/batch = 0.028\n",
      "262/298 (epoch 1), train_loss = 2.223, time/batch = 0.028\n",
      "263/298 (epoch 1), train_loss = 2.253, time/batch = 0.029\n",
      "264/298 (epoch 1), train_loss = 2.182, time/batch = 0.028\n",
      "265/298 (epoch 1), train_loss = 2.245, time/batch = 0.028\n",
      "266/298 (epoch 1), train_loss = 2.244, time/batch = 0.028\n",
      "267/298 (epoch 1), train_loss = 2.257, time/batch = 0.028\n",
      "268/298 (epoch 1), train_loss = 2.252, time/batch = 0.029\n",
      "269/298 (epoch 1), train_loss = 2.202, time/batch = 0.028\n",
      "270/298 (epoch 1), train_loss = 2.176, time/batch = 0.029\n",
      "271/298 (epoch 1), train_loss = 2.186, time/batch = 0.029\n",
      "272/298 (epoch 1), train_loss = 2.220, time/batch = 0.028\n",
      "273/298 (epoch 1), train_loss = 2.182, time/batch = 0.028\n",
      "274/298 (epoch 1), train_loss = 2.174, time/batch = 0.028\n",
      "275/298 (epoch 1), train_loss = 2.252, time/batch = 0.027\n",
      "276/298 (epoch 1), train_loss = 2.171, time/batch = 0.028\n",
      "277/298 (epoch 1), train_loss = 2.242, time/batch = 0.027\n",
      "278/298 (epoch 1), train_loss = 2.232, time/batch = 0.028\n",
      "279/298 (epoch 1), train_loss = 2.230, time/batch = 0.028\n",
      "280/298 (epoch 1), train_loss = 2.228, time/batch = 0.028\n",
      "281/298 (epoch 1), train_loss = 2.227, time/batch = 0.028\n",
      "282/298 (epoch 1), train_loss = 2.262, time/batch = 0.028\n",
      "283/298 (epoch 1), train_loss = 2.228, time/batch = 0.028\n",
      "284/298 (epoch 1), train_loss = 2.186, time/batch = 0.028\n",
      "285/298 (epoch 1), train_loss = 2.228, time/batch = 0.028\n",
      "286/298 (epoch 1), train_loss = 2.245, time/batch = 0.028\n",
      "287/298 (epoch 1), train_loss = 2.195, time/batch = 0.028\n",
      "288/298 (epoch 1), train_loss = 2.257, time/batch = 0.028\n",
      "289/298 (epoch 1), train_loss = 2.248, time/batch = 0.028\n",
      "290/298 (epoch 1), train_loss = 2.255, time/batch = 0.028\n",
      "291/298 (epoch 1), train_loss = 2.256, time/batch = 0.029\n",
      "292/298 (epoch 1), train_loss = 2.200, time/batch = 0.028\n",
      "293/298 (epoch 1), train_loss = 2.248, time/batch = 0.028\n",
      "294/298 (epoch 1), train_loss = 2.195, time/batch = 0.028\n",
      "295/298 (epoch 1), train_loss = 2.215, time/batch = 0.029\n",
      "296/298 (epoch 1), train_loss = 2.201, time/batch = 0.028\n",
      "297/298 (epoch 1), train_loss = 2.215, time/batch = 0.028\n",
      "model saved to save\\model.ckpt\n"
     ]
    }
   ],
   "source": [
    "def train(args):\n",
    "    data_loader = TextLoader(args.data_dir, args.batch_size, args.seq_length)\n",
    "    args.vocab_size = data_loader.vocab_size\n",
    "    \n",
    "    # check compatibility if training is continued from previously saved model\n",
    "    if args.init_from is not None:\n",
    "        # check if all necessary files exist\n",
    "        assert os.path.isdir(args.init_from),\" %s must be a a path\" % args.init_from\n",
    "        assert os.path.isfile(os.path.join(args.init_from,\"config.pkl\")),\"config.pkl file does not exist in path %s\"%args.init_from\n",
    "        assert os.path.isfile(os.path.join(args.init_from,\"chars_vocab.pkl\")),\"chars_vocab.pkl.pkl file does not exist in path %s\" % args.init_from\n",
    "        ckpt = tf.train.latest_checkpoint(args.init_from)\n",
    "        assert ckpt, \"No checkpoint found\"\n",
    "\n",
    "        # open old config and check if models are compatible\n",
    "        with open(os.path.join(args.init_from, 'config.pkl'), 'rb') as f:\n",
    "            saved_model_args = cPickle.load(f)\n",
    "        need_be_same = [\"model\", \"rnn_size\", \"num_layers\", \"seq_length\"]\n",
    "        for checkme in need_be_same:\n",
    "            assert vars(saved_model_args)[checkme]==vars(args)[checkme],\"Command line argument and saved model disagree on '%s' \"%checkme\n",
    "\n",
    "        # open saved vocab/dict and check if vocabs/dicts are compatible\n",
    "        with open(os.path.join(args.init_from, 'chars_vocab.pkl'), 'rb') as f:\n",
    "            saved_chars, saved_vocab = cPickle.load(f)\n",
    "        assert saved_chars==data_loader.chars, \"Data and loaded model disagree on character set!\"\n",
    "        assert saved_vocab==data_loader.vocab, \"Data and loaded model disagree on dictionary mappings!\"\n",
    "\n",
    "    if not os.path.isdir(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "    with open(os.path.join(args.save_dir, 'config.pkl'), 'wb') as f:\n",
    "        cPickle.dump(args, f)\n",
    "    with open(os.path.join(args.save_dir, 'chars_vocab.pkl'), 'wb') as f:\n",
    "        cPickle.dump((data_loader.chars, data_loader.vocab), f)\n",
    "\n",
    "    model = Model(args)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # instrument for tensorboard\n",
    "        summaries = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter(\n",
    "                os.path.join(args.log_dir, time.strftime(\"%Y-%m-%d-%H-%M-%S\")))\n",
    "        writer.add_graph(sess.graph)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        # restore model\n",
    "        if args.init_from is not None:\n",
    "            saver.restore(sess, ckpt)\n",
    "        for e in range(args.num_epochs):\n",
    "            sess.run(tf.assign(model.lr,\n",
    "                               args.learning_rate * (args.decay_rate ** e)))\n",
    "            data_loader.reset_batch_pointer()\n",
    "            state = sess.run(model.initial_state)\n",
    "            for b in range(data_loader.num_batches):\n",
    "                start = time.time()\n",
    "                x, y = data_loader.next_batch()\n",
    "                feed = {model.input_data: x, model.targets: y}\n",
    "                for i, (c, h) in enumerate(model.initial_state):\n",
    "                    feed[c] = state[i].c\n",
    "                    feed[h] = state[i].h\n",
    "\n",
    "                # instrument for tensorboard\n",
    "                summ, train_loss, state, _ = sess.run([summaries, model.cost, model.final_state, model.train_op], feed)\n",
    "                writer.add_summary(summ, e * data_loader.num_batches + b)\n",
    "\n",
    "                end = time.time()\n",
    "                print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\"\n",
    "                      .format(e * data_loader.num_batches + b,\n",
    "                              args.num_epochs * data_loader.num_batches,\n",
    "                              e, train_loss, end - start))\n",
    "                if (e * data_loader.num_batches + b) % args.save_every == 0\\\n",
    "                        or (e == args.num_epochs-1 and\n",
    "                            b == data_loader.num_batches-1):\n",
    "                    # save for the last result\n",
    "                    checkpoint_path = os.path.join(args.save_dir, 'model.ckpt')\n",
    "                    saver.save(sess, checkpoint_path,\n",
    "                               global_step=e * data_loader.num_batches + b)\n",
    "                    print(\"model saved to {}\".format(checkpoint_path))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
